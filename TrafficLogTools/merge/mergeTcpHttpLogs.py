#!/usr/bin/env python
#
# Part of the tool set to process logs generated by PcapEx.
# 
# ***********
# FUNCTION:
# ***********
# Combine the logs of TCP and HTTP by matching their four-tuple
# and considering their timestamps, e.g., a threshold of 60 seconds
# is used to associate a TCP flow in HTTP logs with the flow in TCP logs.
# The output contains a batch of logs:
#    TCP: ...
#    HTTP: ...
#    HTTP: ...
# The first line gives the statistics of a TCP flow and following HTTP
# records mark the HTTP transfers if they are carried by this TCP flow.
# If the TCP flow does not carry HTTP data, then an empty HTTP log is following:
#    TCP: ...
#    HTTP: -
# Likewise, it is possible that the corresponding TCP flow of HTTP transfers is not 
# recorded by the tools, then an empty TCP log is ahead of HTTP recoreds, like
#    TCP: -
#    HTTP: ...
# With the JOIN concept in database theory, this merge operation
# can be considered as a complete outer join in a natural way.
# The complete outer join connects records from two tables with the same socket
# and naturally remove redundent TCP records for multiple HTTP transfers
# carried by a persistent connection.
# 
# **********
# INPUT: 
# **********
# tcp_folder: the folder contains TCP logs. This folder is usually the 
#    position indicated by Tstat command lines.
#    In this folder, some subfolders like "00_00_01_Aug_2013.out"
#    are usually generated which indicates the collection date time.
# http_folder: the folder contains HTTP logs. In this folder,
#    some files named in the form of, e.g., "20130801-0000.jn.out.gz"
#    are generated by the justniffer tool contained in PcapEx.
#
# **********
# OUTPUT:
# **********
# The default output folder is given by TMP_FOLDER in this program.
# The merged logs are given by files for different hours.
#
# **********
# ROLE:
# **********
# The output of this program is intermediate data for further processing 
# with program DPITagTCP.py 
#
# **********
# CONTACT:
# **********
# By chenxm
# chenxm35@gmail.com
# Created: 2013-03-13
# Updated: 2013-08-27
#
import os
import sys
import uuid
import re
import logging
import time, datetime
import socket, struct
import csv, StringIO

from omnipy.utils import gzip_mod
from omnipy.data.reader import FileReader

# File name patterns
TMP_FOLDER = "./merged-tcp-http-tmp"
OUT_FILE_SUFFIX = '-merged.tmp'
TCP_TIME_PATTERN = "%H_%M_%d_%b_%Y"
HTTP_FN_PATTERN = r'\d{8}-\d{4}\.jn.'
HTTP_TIME_PATTERN = "%Y%m%d-%H%M"

def print_usage():
    print "Usage: program <tcp_folder> <http_folder>"
    print "       'tcp_folder' refers to Tstat output folder"
    print "       'http_folder' means justniffer output folder"


## Parsing options
if len(sys.argv) == 3:
    input_tcp_log = sys.argv[1]
    input_http_log = sys.argv[2]
else:
    print_usage()
    exit(-1)

#logger
logging.basicConfig(filename=os.path.basename(sys.argv[0]).rsplit('.',1)[0]+'.log',
    level=logging.DEBUG)

try: os.mkdir(TMP_FOLDER)
except: pass


WIN_LIMIT = 5 * 60 # 10 minutes

class _TWindow(object):
    def __init__(self):
        self._ts = None
        self._te = None

    @property
    def ts(self):
        return self._ts

    @property
    def te(self):
        return self._te

    @ts.setter
    def ts(self, value):
        assert isinstance(value, float)
        self._ts = value

    @te.setter
    def te(self, value):
        assert isinstance(value, float)
        self._te = value

    def width(self):
        return self._te-self._ts

    def contains(self, tvalue):
        assert isinstance(tvalue, float)
        return True if (tvalue <= self._te and tvalue >= self._ts) else False


def ip2int(s):
    "Convert dotted IPv4 address to integer."
    try:
        return reduce(lambda a,b: a<<8 | b, map(int, s.split(".")))
    except:
        raise ValueError
 
def int2ip(ip):
    "Convert 32-bit integer to dotted IPv4 address."
    try:
        return ".".join(map(lambda n: str(ip>>n & 0xFF), [24,16,8,0]))
    except:
        raise ValueError

def _is_valid_http(http_props):
    # check with protocol name at index 19
    try:
        return True if "HTTP" in http_props[19][:9] else False
    except IndexError:
        return False

def _is_valid_tcp(tcp_props):
    # check connection time at index 97
    try:
        return True if len(tcp_props) >= 110 else False
    except:
        return False

def _fix_time(timestr):
    timeval = None
    if '.' not in timestr:
        microsecs = '0'
    else:
        secs, microsecs = timestr.split('.', 1)
    try:
        timeval = float(secs)
        if len(microsecs) > 0:
            timeval += float(microsecs)/1000000.0
    except ValueError:
        pass
    finally:
        return timeval

def _fix_time2(timestr):
    timeval = None
    if '.' not in timestr:
        microsecs = '0'
    else:
        millisecs, microsecs = timestr.split('.', 1)
    try:
        timeval = float(millisecs)/1000.0
        if len(microsecs) > 0:
            timeval += float(microsecs)/1000000.0
    except ValueError:
        pass
    finally:
        return timeval


def _read_win_http(http_transfers, fileobj, tm_win):
    eof = False
    full = False
    total_http_trans = 0
    while not full:
        try:
            next = fileobj.next()
            sio = csv.reader(StringIO.StringIO(next), 
                delimiter = ' ', quotechar = '\"', quoting=csv.QUOTE_MINIMAL)
            try:
                http_props = sio.next()
            except csv.Error as e:
                continue

            if not _is_valid_http(http_props):
                continue
            else:
                total_http_trans += 1

            conn_type = http_props[4]
            if conn_type not in ['start', 'continue', 'last', 'unique']:
                continue
                
            conn_time = None
            if conn_type in ["start", "unique"]:
                conn_time = _fix_time(http_props[5]) # in seconds
                if conn_time and conn_time > tm_win.te and not tm_win.contains(conn_time):
                    full = True
            
            req_time = _fix_time(http_props[9]) # in seconds
            if not req_time:
                continue # make sure that the request time is valid for all requests.

            try:
                src_addr = ip2int(http_props[0])
                dst_addr = ip2int(http_props[2])
                src_port = int( http_props[1] )
                dst_port = int( http_props[3] )
            except (ValueError, IndexError):
                continue

            tkey = sorted([(src_addr, src_port),  (dst_addr, dst_port)])
            tkey = tuple(tkey)
            if tkey not in http_transfers:
                http_transfers[tkey] = []

            http_transfers[tkey].append( (conn_type, conn_time, req_time, next) )
            http_transfers[tkey].sort(lambda x,y: cmp(x[2], y[2]), None, False) # By request time
        except StopIteration:
            eof = True
            break
    return eof, total_http_trans

def _read_win_tcp(tcp_flows, fileobj, complete, tm_win):
    eof = False
    full = False
    total_tcp_flows = 0
    while not full:
        try:
            next = fileobj.next()
            sio = csv.reader(StringIO.StringIO(next), delimiter = ' ')
            try:
                tcp_parts = sio.next()
            except csv.Error as e:
                continue

            if not _is_valid_tcp(tcp_parts): continue
            else: total_tcp_flows += 1

            conn_time = _fix_time2(tcp_parts[97]) # in seconds
            if conn_time and not tm_win.contains(conn_time) and conn_time > tm_win.te:
                full = True

            try:
                src_addr = ip2int(tcp_parts[0])
                src_port = int( tcp_parts[1] )
                dst_addr = ip2int(tcp_parts[44])
                dst_port = int( tcp_parts[45] )
            except (ValueError, IndexError):
                continue

            tkey = sorted([(src_addr, src_port),  (dst_addr, dst_port)])
            tkey = tuple(tkey)
            if tkey not in tcp_flows:
                tcp_flows[tkey] = []

            tcp_flows[tkey].append( (complete, conn_time, next) )
            tcp_flows[tkey].sort(lambda x,y: cmp(x[1], y[1]), None, False) # by connection time
        except StopIteration:
            eof = True
            break
    return eof, total_tcp_flows


def _do_match(tcp_flows, http_transfers, ofile, socket, transfers):
    # DO MATCH
    matched = False
    matched_flows = []
    matched_transfers = []
    align_ts = transfers[0][1] if transfers[0][1] else transfers[0][2]
    for flow in tcp_flows[socket]:
        if abs(flow[1] - align_ts) < 180: # seconds
            ofile.write("TCP: %s\n" % flow[2].strip('\r \n'))
            matched_flows.append(flow)
            matched = True
    if matched:
        # remove matched HTTP transfers
        for trans in transfers:
            ofile.write("HTTP: %s\n" % trans[3].strip('\r \n'))
            matched_transfers.append(trans)
    # remove matched TCP flows
    tcp_flows[socket] = [i for i in tcp_flows[socket] if i not in matched_flows]
    http_transfers[socket] = [i for i in http_transfers[socket] if i not in matched_transfers]
    return matched


def _merge_window(tcp_flows, http_transfers, ofile):
    matched_sockets = set()
    for socket in http_transfers:
        if len(http_transfers[socket]) > 0 and (socket in tcp_flows):
            persist_conn = []
            persist_finish = False
            for transfer in http_transfers[socket]:
                if transfer[0] == 'last':
                    persist_conn.append(transfer)
                    persist_finish = True
                elif transfer[0] == 'unique':
                    persist_conn = [transfer]
                    persist_finish = True
                elif transfer[0] == 'continue':
                    persist_conn.append(transfer)
                elif transfer[0] == 'start':
                    persist_conn = [transfer]
                else:
                    pass
                if persist_finish:
                    if _do_match(tcp_flows, http_transfers, ofile, socket, persist_conn):
                        matched_sockets.add(socket)
                    persist_conn = []
    for socket in matched_sockets:
        if len(tcp_flows[socket]) == 0:
            del tcp_flows[socket]
        if len(http_transfers[socket]) == 0:
            del http_transfers[socket]

def _dump_window(tcp_flows, http_transfers, ofile):
    # Final match
    for socket in http_transfers:
        if len(http_transfers[socket]) > 0 and (socket in tcp_flows):
            persist_conn = []
            persist_finish = False
            for transfer in http_transfers[socket]:
                if transfer[0] == 'continue':
                    persist_conn.append(transfer)
                elif transfer[0] == 'start':
                    if len(persist_conn) == 0:
                        persist_conn = [transfer]; continue
                    _do_match(tcp_flows, http_transfers, ofile, socket, persist_conn)
                    persist_conn = [transfer]
            if len(persist_conn) > 0:
                _do_match(tcp_flows, http_transfers, ofile, socket, persist_conn)
    # Dump no-matches
    http_trans_without_tcp_flow = 0
    for socket in http_transfers:
        if len(http_transfers[socket]) > 0:
            ofile.write("TCP: -\n")
            for transfer in http_transfers[socket]:
                ofile.write("HTTP: %s\n" % transfer[3].strip('\r \n'))
                http_trans_without_tcp_flow += 1
    for socket in tcp_flows:
        for flow in tcp_flows[socket]:
            ofile.write("TCP: %s\n" % flow[2].strip('\r \n'))
            ofile.write("HTTP: -\n")
    return http_trans_without_tcp_flow

def _merge_http_tcp(http_file, tcp_file, dt):
    """
    Merge single HTTP and TCP files
    """
    tcp_file_complete = os.path.join(tcp_file, "log_tcp_complete.gz")
    #tcp_file_nocomplete = os.path.join(tcp_file, "log_tcp_nocomplete.gz")

    ihttp_file = FileReader.open_file(http_file, 'rb')
    itcp_file_complete = FileReader.open_file(tcp_file_complete, 'rb')

    tm_win = _TWindow()
    tm_win.ts = time.mktime(dt.timetuple())
    tm_win.te = tm_win.ts + WIN_LIMIT
    http_transfers = {}    # {socket: [(conn_type, conn_time, req_time, content)]}
    tcp_flows = {} # {socket: [(complete, conn_time, content)]}
    total_http_trans = 0
    total_tcp_flows = 0
    http_trans_without_tcp_flow = 0
        
    eof_http = False
    eof_tcp = False
    ofile = os.path.join(TMP_FOLDER, dt.strftime(HTTP_TIME_PATTERN)+OUT_FILE_SUFFIX)
    #ofile = gzip_mod.GzipFile(filename=ofile, mode='wb')
    ofile = open(ofile, mode='wb')

    while not (eof_http and eof_tcp):
        eof_http, cnt = _read_win_http(http_transfers, ihttp_file, tm_win)
        total_http_trans += cnt
        eof_tcp, cnt = _read_win_tcp(tcp_flows, itcp_file_complete, True, tm_win)

        _merge_window(tcp_flows, http_transfers, ofile)
        # Update window
        tm_win.ts += tm_win.te
        tm_win.te += WIN_LIMIT

    http_trans_without_tcp_flow += _dump_window(tcp_flows, http_transfers, ofile)

    print("%d/%d HTTP transfers without TCP flows" % (http_trans_without_tcp_flow, total_http_trans))

    ofile.close()
    ihttp_file.close()
    itcp_file_complete.close()
    logging.info('file for %s completed' % dt)

NUM_WORKERS = 1;
def multipleThread(hour_files_map):
    if not isinstance(hour_files_map, dict):
        print("ERROR: the input must be a map.")
        return;

    import Queue
    from threading import Thread

    q = Queue.Queue()
    for item in sorted(hour_files_map.items()):
        q.put(item)

    def worker():
        while True:
            key, value = q.get()
            print(key)
            http_file = value[0]
            tcp_file = value[1]
            _merge_http_tcp(http_file, tcp_file, key)
            q.task_done()	# tell the queue

    for i in range(NUM_WORKERS):
        t = Thread(target=worker)
        t.daemon = True
        t.start()

    q.join()


def do_merge(http_folder, tcp_folder):
    # Group HTTP and TCP w.r.t. the same hour
    hour_files_map = {} # {hour: [http, tcp]}
    files = FileReader.list_files(http_folder, HTTP_FN_PATTERN)
    for file in files:
        bname = os.path.basename(file)
        dt = datetime.datetime.strptime(bname.split('.',1)[0], HTTP_TIME_PATTERN)
        dt = dt.replace(minute=0, second=0)
        if dt not in hour_files_map:
            hour_files_map[dt] = [file, None]
        else:
            print("WARNNING: Hour %s occurs with more one HTTP file" % str(dt))

    subfolders = os.listdir(tcp_folder)
    for sfolder in subfolders:
        try:
            dt = datetime.datetime.strptime(sfolder.split('.',1)[0], TCP_TIME_PATTERN)
        except ValueError:
            continue
        dt = dt.replace(minute=0, second=0)
        if dt in hour_files_map:
            hour_files_map[dt][1] = os.path.join(tcp_folder, sfolder)
        else:
            hour_files_map[dt] = [None, os.path.join(tcp_folder, sfolder)]
            print("WARNNING: hour %s occurs without HTTP data found" % str(dt))

    multipleThread(hour_files_map)

# Do merge
do_merge(input_http_log, input_tcp_log)
